{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word and Position Embedding implemetation of Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2, 3],\n",
      "        [0, 1, 2, 3]], dtype=torch.int32)\n",
      "tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "           1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
      "         [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
      "           9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
      "         [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
      "           9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
      "         [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,\n",
      "           9.9955e-01,  3.0000e-03,  1.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "           1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
      "         [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
      "           9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
      "         [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
      "           9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
      "         [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,\n",
      "           9.9955e-01,  3.0000e-03,  1.0000e+00]]])\n",
      "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "          1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
      "        [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
      "          9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
      "        [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
      "          9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
      "        [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,\n",
      "          9.9955e-01,  3.0000e-03,  1.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "# for word embedding\n",
    "batch_size = 2\n",
    "# size of word table \n",
    "max_num_src_words = 8\n",
    "max_num_tgt_words = 8\n",
    "# model dimension\n",
    "model_dim = 8\n",
    "#max length of sequence\n",
    "#max_src_seq_len = 5\n",
    "#max_tgt_seq_len = 5\n",
    "max_position_len = 0\n",
    "\n",
    "\n",
    "src_len = torch.Tensor([2,4]).to(torch.int32)\n",
    "tgt_len = torch.Tensor([4,3]).to(torch.int32)\n",
    "\n",
    "# sentenced constructed from batch of word index, and pad with 0\n",
    "src_seq = torch.cat([torch.unsqueeze(F.pad(torch.randint(1,max_num_src_words,(L,)),(0,max(src_len)-L)),0) for L in src_len])\n",
    "tgt_seq = torch.cat([torch.unsqueeze(F.pad(torch.randint(1,max_num_src_words,(L,)),(0,max(tgt_len)-L)),0) for L in tgt_len])\n",
    "max_position_len = max(src_len)\n",
    "#test = [torch.unsqueeze(F.pad(torch.randint(1,max_num_src_words,(L,)),(0,max_src_seq_len-L)),0) for L in src_len]\n",
    "#print(type(test))\n",
    "#print(src_seq)\n",
    "#print(tgt_seq)\n",
    "\n",
    "'''word embedding construction'''\n",
    "src_embedding_table = nn.Embedding(max_num_src_words+1,model_dim) # +1 for padding \n",
    "tgt_embedding_table = nn.Embedding(max_num_tgt_words+1,model_dim)\n",
    "\n",
    "src_embedding = src_embedding_table(src_seq) # [batch*max_src_len*dim]  2*4*8\n",
    "tgt_embedding = src_embedding_table(tgt_seq)\n",
    "\n",
    "#print(src_embedding_table.weight) # 9*8, the first row represent padding\n",
    "##print(src_embedding)\n",
    "\n",
    "'''postion embedding construction'''\n",
    "pos_mat = torch.arange(max(src_len)).reshape(-1,1) # every row is the same, the position of the token in the sequence [4,1]\n",
    "i_mat = torch.arange(0,model_dim,2).reshape(1,-1)/model_dim  # every column is the same, the dimension  [1,4]\n",
    "i_mat = torch.pow(10000,i_mat)\n",
    "\n",
    "pe_embedding_table = torch.zeros(max_position_len,model_dim) #[4,8]\n",
    "pe_embedding_table[:,0::2] = torch.sin(pos_mat/i_mat) #pos_mat/i_mat = [4,4]\n",
    "pe_embedding_table[:,1::2] = torch.cos(pos_mat/i_mat) \n",
    "# wrap the pe embedding table to embedding layer\n",
    "pe_embedding = nn.Embedding(max_position_len,model_dim)\n",
    "pe_embedding.weight = nn.Parameter(pe_embedding_table,requires_grad=False)\n",
    "\n",
    "src_pos = torch.cat([torch.unsqueeze(torch.arange(max(src_len)),0) for _ in src_len]).to(torch.int32) #[batch,max(scr_len)] 2*4\n",
    "tgt_pos = torch.cat([torch.unsqueeze(torch.arange(max(tgt_len)),0) for _ in tgt_len]).to(torch.int32)\n",
    "print(src_pos)\n",
    "src_pe_embedding = pe_embedding(src_pos)#batch*max(src_len)*dim\n",
    "tgt_pe_embedding = pe_embedding(tgt_pos)\n",
    "print(src_pe_embedding)\n",
    "#print(pos_mat)\n",
    "#print(i_mat)\n",
    "print(pe_embedding_table)\n",
    "#print(pe_embedding.weight)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0395, -0.0062, -0.0055, -0.0025, -0.0253],\n",
      "        [-0.0062,  0.1283, -0.0200, -0.0091, -0.0929],\n",
      "        [-0.0055, -0.0200,  0.1149, -0.0080, -0.0815],\n",
      "        [-0.0025, -0.0091, -0.0080,  0.0566, -0.0370],\n",
      "        [-0.0253, -0.0929, -0.0815, -0.0370,  0.2368]])\n",
      "tensor([-1.3812, -0.0819, -0.2138, -1.0017,  1.3214])\n",
      "tensor([0.0412, 0.1511, 0.1325, 0.0602, 0.6149])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_s/qf2hpr911z5167gwmr6bb1940000gn/T/ipykernel_29846/2366350606.py:7: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(score)\n"
     ]
    }
   ],
   "source": [
    "# reason why we need scaled before softmax\n",
    "alpha1 = 0,1\n",
    "alpha2 = 10\n",
    "score = torch.randn(5)\n",
    "prob = F.softmax(score,-1)\n",
    "def softmax_func(score):\n",
    "    return F.softmax(score)\n",
    "jaco_mat = torch.autograd.functional.jacobian(softmax_func,score)\n",
    "print(jaco_mat)\n",
    "print(score)\n",
    "print(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# encoder self-attention mask construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1.]])\n",
      "torch.Size([2, 4, 4])\n",
      "torch.Size([2, 4, 4])\n",
      "tensor([[[0.2545, 0.7455, 0.0000, 0.0000],\n",
      "         [0.7336, 0.2664, 0.0000, 0.0000],\n",
      "         [0.2500, 0.2500, 0.2500, 0.2500],\n",
      "         [0.2500, 0.2500, 0.2500, 0.2500]],\n",
      "\n",
      "        [[0.1371, 0.2022, 0.0548, 0.6058],\n",
      "         [0.2571, 0.2074, 0.3833, 0.1522],\n",
      "         [0.1210, 0.0433, 0.2453, 0.5905],\n",
      "         [0.5462, 0.0928, 0.1048, 0.2563]]])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "shape of mask: [batch_size, max_src_len,max_src_len],\n",
    "value is 1 or -inf\n",
    "'''\n",
    "valid_encoder_pos = torch.cat([torch.unsqueeze(F.pad(torch.ones(L),(0,max(src_len)-L)),0) for L in src_len])\n",
    "print(valid_encoder_pos)\n",
    "valid_encoder_pos = torch.unsqueeze(valid_encoder_pos,2) #[batch_size,max(src_len),1]\n",
    "#print(valid_encoder_pos.shape)\n",
    "valid_encoder_pos_matrix = torch.bmm(valid_encoder_pos,valid_encoder_pos.transpose(1,2))\n",
    "#print(valid_encoder_pos_matrix)\n",
    "invalid_encoder_pos_matrix = 1-valid_encoder_pos_matrix\n",
    "#print(invalid_encoder_pos_matrix)\n",
    "mask_encoder_self_attention = invalid_encoder_pos_matrix.to(torch.bool)\n",
    "#print(mask_encoder_self_attention.shape)\n",
    "\n",
    "score = torch.randn(batch_size,max(src_len),max(src_len))\n",
    "#print(score)\n",
    "masked_score = score.masked_fill(mask_encoder_self_attention,-1e9)\n",
    "#print(masked_score.shape)\n",
    "prob = F.softmax(masked_score,-1)\n",
    "#print(prob)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[False,  True,  True,  True],\n",
      "         [False, False,  True,  True],\n",
      "         [False, False, False,  True],\n",
      "         [False, False, False, False]],\n",
      "\n",
      "        [[False,  True,  True,  True],\n",
      "         [False, False,  True,  True],\n",
      "         [False, False, False,  True],\n",
      "         [ True,  True,  True,  True]]])\n",
      "tensor([[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.6494, 0.3506, 0.0000, 0.0000],\n",
      "         [0.5234, 0.1916, 0.2849, 0.0000],\n",
      "         [0.1583, 0.3316, 0.3902, 0.1199]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.7329, 0.2671, 0.0000, 0.0000],\n",
      "         [0.0760, 0.1678, 0.7562, 0.0000],\n",
      "         [0.2500, 0.2500, 0.2500, 0.2500]]])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "cross-attention mask A@K^T shape: [batch_size, tgt_seq_len,src_seq_len]\n",
    "'''\n",
    "valid_encoder_pos = torch.cat([torch.unsqueeze(F.pad(torch.ones(L),(0,max(src_len)-L)),0) for L in src_len])\n",
    "#print(valid_encoder_pos)\n",
    "valid_encoder_pos = torch.unsqueeze(valid_encoder_pos,2) #[batch_size,max(src_len),1]\n",
    "\n",
    "valid_decoder_pos = torch.cat([torch.unsqueeze(F.pad(torch.ones(L),(0,max(tgt_len)-L)),0) for L in tgt_len])\n",
    "\n",
    "#print(valid_encoder_pos)\n",
    "valid_decoder_pos = torch.unsqueeze(valid_decoder_pos,2) #[batch_size,max(src_len),1]\n",
    "\n",
    "valid_cross_pos_matrix = torch.bmm(valid_decoder_pos,valid_encoder_pos.transpose(1,2))\n",
    "#print(valid_decoder_pos.shape)\n",
    "invalid_cross_pos_matrix = 1-valid_cross_pos_matrix\n",
    "mask_cross_attention = invalid_encoder_pos_matrix.to(torch.bool)\n",
    "#print(valid_cross_pos_matrix)\n",
    "#print(invalid_cross_pos_matrix)\n",
    "#print(mask_cross_attention)\n",
    "\n",
    "'''\n",
    "Decoder self-attention mask\n",
    "'''\n",
    "valid_decoder_tri_matrix = torch.cat([torch.unsqueeze(F.pad(torch.tril(torch.ones(L,L)),(0,max(tgt_len)-L,0,max(tgt_len)-L)),0) for L in tgt_len])\n",
    "#print(valid_decoder_tri_matrix)\n",
    "invalid_decoder_tri_matrix = 1-valid_decoder_tri_matrix\n",
    "invalid_decoder_tri_matrix = invalid_decoder_tri_matrix.to(torch.bool)\n",
    "print(invalid_decoder_tri_matrix)\n",
    "\n",
    "score = torch.randn(batch_size,max(tgt_len),max(tgt_len))\n",
    "masked_score = score.masked_fill(invalid_decoder_tri_matrix,-1e9)\n",
    "prob = F.softmax(masked_score,-1)\n",
    "print(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q,K,V,atten_mask):\n",
    "    # shape of Q,K,V: (batch_size*num_head,seq_len,model_dim/num_head)\n",
    "    score = torch.bmm(Q,K.transpose(-2,-1))/torch.sqrt(model_dim)\n",
    "    masked_score = score.masked_fill( atten_mask,-1e9)\n",
    "    prob = F.softmax(masked_score,-1)\n",
    "    context = torch.bmm(prob,V)\n",
    "    return context"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
